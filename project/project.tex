%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%% ICML 2015 EXAMPLE LATEX SUBMISSION FILE %%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% Use the following line _only_ if you're still using LaTeX 2.09.
%\documentstyle[icml2015,epsf,natbib]{article}
% If you rely on Latex2e packages, like most moden people use this:
\documentclass{article}

% use Times
\usepackage{times}
% For figures
\usepackage{graphicx} % more modern
%\usepackage{epsfig} % less modern
\usepackage{subfigure} 

% For citations
\usepackage{natbib}

% For algorithms
\usepackage{algorithm}
\usepackage{algorithmic}

% As of 2011, we use the hyperref package to produce hyperlinks in the
% resulting PDF.  If this breaks your system, please commend out the
% following usepackage line and replace \usepackage{icml2015} with
% \usepackage[nohyperref]{icml2015} above.
\usepackage{hyperref}

% Packages hyperref and algorithmic misbehave sometimes.  We can fix
% this with the following command.
\newcommand{\theHalgorithm}{\arabic{algorithm}}


% Employ the following version of the ``usepackage'' statement for
% submitting the draft version of the paper for review.  This will set
% the note in the first column to ``Under review.  Do not distribute.''
\usepackage[accepted]{icml2015}

% Employ this version of the ``usepackage'' statement after the paper has
% been accepted, when creating the final version.  This will set the
% note in the first column to ``Proceedings of the...''
%\usepackage[accepted]{icml2015}


% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
\icmltitlerunning{Submission and Formatting Instructions for ICML 2015}

\let\oldthebibliography=\thebibliography
  \let\endoldthebibliography=\endthebibliography
  \renewenvironment{thebibliography}[1]{%
    \begin{oldthebibliography}{#1}%
      \setlength{\parskip}{.3ex}%
      \setlength{\itemsep}{.3ex}%
  }%
  {%
    \end{oldthebibliography}%
  }
  
\begin{document} 
\twocolumn[
\icmltitle{CS 6780 Research Proposal: Top N Recommender Systems}

% It is OKAY to include author information, even for blind
% submissions: the style file will automatically remove it for you
% unless you've provided the [accepted] option to the icml2015
% package.
\icmlauthor{Bangrui Chen}{bc496@cornell.edu}
\icmlauthor{Saul Toscano Palmerin}{st684@cornell.edu}
\icmlauthor{Zhengdi Shen}{zs267@cornell.edu}

% You may provide any keywords that you 
% find helpful for describing your paper; these are used to populate 
% the "keywords" metadata in the PDF but will not be shown in the document
\icmlkeywords{boring formatting information, machine learning, ICML}

\vskip 0.3in
]


\section{Motivation}
We present a greedy Bayesian approach to solve the contextual bandit problem, and we study how well this method does. We will do numerical experiments and we might be able to prove some performance result.


\section{Problem Formulation}
In this problem, we are given a collection of items (such as movies, books, articles, etc), denote them as $I=\{I_{1},I_{2},\cdots,I_{M}\}$. For each item $I_{n}$, we use a k dimension vector $I_{n}=(i_{1,n},\cdots,i_{k,n})$ to represent its score on k different features. For any time $t=1,2,\cdots,T$, we are asked to pick N items $S_{t}=\{I_{t,1},I_{t,2},\cdots,I_{t,N}\}\subset I$, order them and forward these N items to the user. 

We assume there is a known probability vector $(p_{1},\cdots,p_{N})$ associated with the N different positions. Since the top positions are more likely to be seen by the user, we further assume $p_{1}\geq p_{2},\cdots\geq p_{N}$. The user has a user preference vector $\theta=\{\theta_{1},\cdots,\theta_{k}\}$ which is unknown but remain static over the time. Every time the user sees N items, he/she will click the item $I_{t,i}$ with probability $\left(1+\mbox{exp}\left(-p_{i}\theta\cdot I_{t,i}\right)\right)^{-1}$. If the user clicks the item, then we receive a reward 1 and 0 otherwise. Thus, the feedback from the user in this case is a N dimensional vector $(Y_{1,n},\cdots,Y_{N,n})$ and $Y_{i,n}=1$ if the users clicks the item on the $i_{th}$ position and 0 otherwise.  For each $Y_{i,n}$, we know 
\begin{equation}
Y_{i,n}|\theta,I_{t,i},p_{i}\sim Bernoulli(\frac{1}{1+\mbox{exp}\left(-p_{i}\theta\cdot I_{t,i}\right)}). \nonumber 
\end{equation}
Denote the total reward received at time n as $Y_{n}=\sum_{i=1}^{N}Y_{i,n}$.The goal of this problem is to find a strategy $\pi$ to maximize the following expression
\begin{equation}
E^{\pi}\left[\sum_{t=1}^{T} Y_{t}\right]
\end{equation}

The questions we are going to solve are the following: 

(1) Suppose there are many historical users in our database and we know their user preference vectors. Then we can calculate the empirical distribution for those user preference vectors and treat it as the prior distribution of the current $\theta$. Every time we observe the user's feedback $(Y_{1,n},\cdots,Y_{N,n})$, we can use this feedback and maximum likelihood method to find a new estimate for $\theta$, denote as $\hat{\theta}_{n}$. Then we make our next selection based on this $\hat{\theta}_{n}$. How good is this greedy method compared to other existing methods? 

(2) Further, when we make the selection based on $\hat{\theta}_{n}$, what's the best trade off between exploitation and exploration?

(3) When k is large, this problem is computational intractable. How could we solve this problem efficiently? Can we prove any structural results for this problem?


\section{Possible Applications}

Recommender systems want to find the preference that a user would give to a subset of a finite set of items. They're widely applied to different problems. For example, they're used in Netflix where there are thousands of movies and TV episodes. The biggest challenge of these problems is that there are millions of objets and hundreds of million of users, and so it's necessary to find a model that performs well and be sufficiently fast.


\section{Approach and Recourses}
We will develop our method and test the performance on the dataset provided by companies like Yelp or Netflix.


\section{Schedule}

 March 1: Test the performance of greedy method.\\
 March 23: Trade off between expatiation and exploration.\\
 April 20: Improve the efficiency for large data.\\
 May 11: Finish the report.
 
 
\begin{thebibliography}[1]
X Zhao, P Frazier, \emph{Exploration vs. Exploitation in the Information Filtering Problem}
\end{thebibliography}

\end{document} 


% This document was modified from the file originally made available by
% Pat Langley and Andrea Danyluk for ICML-2K. This version was
% created by Lise Getoor and Tobias Scheffer, it was slightly modified  
% from the 2010 version by Thorsten Joachims & Johannes Fuernkranz, 
% slightly modified from the 2009 version by Kiri Wagstaff and 
% Sam Roweis's 2008 version, which is slightly modified from 
% Prasad Tadepalli's 2007 version which is a lightly 
% changed version of the previous year's version by Andrew Moore, 
% which was in turn edited from those of Kristian Kersting and 
% Codrina Lauth. Alex Smola contributed to the algorithmic style files.  
